{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets.mnist1d import load_MNIST1D\n",
    "from datasets.control import get_mixture_distribution, load_control\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b458fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture_list = [\n",
    "#     # uniform between [-10, -5] and [5, 10] (for 2D data)\n",
    "#     (\"uniform\",\n",
    "#      np.array([-10.0, -5.0, -10, -10]),\n",
    "#      np.array([5.0, 10.0, -5, 5])),\n",
    "\n",
    "#     # normal with mean [10, 5, -5] and std [2, 12, 1]\n",
    "#     (\"normal\",\n",
    "#      np.array([10.0,  5.0, -5.0, -5]),\n",
    "#      np.array([ 2.0, 12.0,  1.0, 1])),\n",
    "\n",
    "#     # normal centered at 0 with unit variance in all dims\n",
    "#     (\"normal\",\n",
    "#      np.array([0.0, 0.0, 0.0, 0]),\n",
    "#      np.array([1.0, 1.0, 1.0, 1])),\n",
    "\n",
    "#     # normal with mean [3, 6, 2] and std [4, 5, 3]\n",
    "#     (\"normal\",\n",
    "#      np.array([3.0, 6.0, 2.0, 2]),\n",
    "#      np.array([4.0, 5.0, 3.0, 3])),\n",
    "\n",
    "#     # laplace centered at [4, 4, 4] with scale 1\n",
    "#     (\"laplace\",\n",
    "#      np.array([4.0, 4.0, 4.0, 4]),\n",
    "#      np.array([1.0, 1.0, 1.0, 1])),\n",
    "\n",
    "#     # normal centered at -5 with std 3 in all dims\n",
    "#     (\"normal\",\n",
    "#      np.array([-5.0, -5.0, -5.0, -5]),\n",
    "#      np.array([3.0, 3.0, 3.0, 3])),\n",
    "\n",
    "#     # exponential with scale=1 in all dims, then shifted by -5\n",
    "#     (\"exponential\",\n",
    "#      np.array([1.0, 1.0, 1.0, 1]),    # scale\n",
    "#      np.array([-5.0, -5.0, -5.0, -5])), # shift\n",
    "# ]\n",
    "\n",
    "\n",
    "mixture_list = [\n",
    "    ('uniform', np.array([-10.,  -5.]), np.array([ 5., 10.])),\n",
    "    ('normal', np.array([10.,  5.]), np.array([ 2., 12.])),\n",
    "    ('normal', np.array([0., 0.]), np.array([1., 1.])),\n",
    "    ('normal', np.array([3., 6.]), np.array([4., 5.])),\n",
    "    ('laplace', np.array([4., 4.]), np.array([1., 1.])),\n",
    "    ('normal', np.array([-5., -5.]), np.array([3., 3.])),\n",
    "    ('exponential', np.array([1., 1.]), np.array([-5., -5.]))\n",
    "]\n",
    "\n",
    "# mixture_list = [\n",
    "#     ('normal', np.array([0., 0.]), np.array([1., 1.])),\n",
    "# ]\n",
    "\n",
    "weights = [0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.1]\n",
    "# weights = [1]\n",
    "control_data = get_mixture_distribution(mixture_list, weights, size=(10000, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0072f",
   "metadata": {},
   "source": [
    "# Train GMMN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cddc9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model_weights\"\n",
    "\n",
    "BATCH_SIZE = 1000 # taken from original paper\n",
    "# trainloader, testloader, min_value, max_value = load_MNIST1D(batch_size = BATCH_SIZE)\n",
    "# trainloader, testloader = load_MNIST(batch_size = BATCH_SIZE, size=7, flatten=True)\n",
    "trainloader, testloader, min_value, max_value = load_control(control_data, batch_size=1000)\n",
    "for batch_inputs, batch_targets in trainloader:\n",
    "    # Do training step here\n",
    "    print(batch_inputs.shape, batch_targets.shape)\n",
    "    break\n",
    "N_INP = next(iter(trainloader))[0].shape[1]\n",
    "NOISE_SIZE = 10\n",
    "ENCODED_SIZE = 10\n",
    "N_ENCODER_EPOCHS = 1000\n",
    "N_GEN_EPOCHS = 500\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b952c",
   "metadata": {},
   "source": [
    "## Train Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gmmn.train_autoencoder import train_autoencoder\n",
    "from visualization.loss import plot_loss\n",
    "\n",
    "ENCODER_SAVE_PATH = model_dir + \"/autoencoder_control-multivariate-gaussian.pth\"\n",
    "autoencoder, losses_autoencoder = train_autoencoder(trainloader, N_INP, ENCODED_SIZE, N_ENCODER_EPOCHS, device, ENCODER_SAVE_PATH)\n",
    "plot_loss(losses_autoencoder, title=\"Autoencoder Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda99ece",
   "metadata": {},
   "source": [
    "## Continue with GMMN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gmmn.train_gmmn import train_gmmn\n",
    "sigmas = [0.01, 0.1, 0.5,  1, 2,  5, 10, 20, 50]\n",
    "GMMN_SAVE_PATH = model_dir + \"/gmmn_standard_normal_expanded_sigmas.pth\"\n",
    "\n",
    "gmm_net, losses_gmmn = train_gmmn(\n",
    "    trainloader, autoencoder, sigmas, ENCODED_SIZE, NOISE_SIZE,\n",
    "    BATCH_SIZE, N_GEN_EPOCHS, device, GMMN_SAVE_PATH\n",
    ")\n",
    "\n",
    "plot_loss(losses_gmmn, title=\"GMMN Loss with Expanded Sigmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd81914",
   "metadata": {},
   "source": [
    "## Sample Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69ff1e",
   "metadata": {},
   "source": [
    "##### MNIST 1-D\n",
    "**Run only if data used is MNIST-1D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.visualize_1d_data import visualize_mnist1d\n",
    "from models.gmmn.gmmn import generate_gmmn_samples\n",
    "\n",
    "samples, labels = next(iter(trainloader))\n",
    "visualize_mnist1d(samples, labels, title_prefix=\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samples = generate_gmmn_samples(gmm_net, autoencoder, NOISE_SIZE, 10)\n",
    "visualize_mnist1d(gen_samples, labels, title_prefix=\"Generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37523",
   "metadata": {},
   "source": [
    "##### Control Data\n",
    "**Work only if working with control data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization.plots\n",
    "visualization.plots.plot_3d_kde(control_data, \"Original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = []\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "        reconstructed_data.append(autoencoder.forward(data)[1])\n",
    "# print(reconstructed_data[0][0].shape)\n",
    "reconstructed_data = torch.cat(reconstructed_data, dim=0).numpy()\n",
    "reconstructed_data = np.add(np.multiply(max_value - min_value, reconstructed_data), min_value)\n",
    "# reconstructed_data = np.array(reconstructed_data)\n",
    "visualization.plots.plot_3d_kde(reconstructed_data, \"Reconstructed Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2329d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gmmn.gmmn import generate_gmmn_samples\n",
    "gen_samples = generate_gmmn_samples(gmm_net, autoencoder, NOISE_SIZE, 10000)\n",
    "gen_samples = np.add(np.multiply(max_value - min_value, gen_samples), min_value)\n",
    "visualization.plots.plot_3d_kde(gen_samples, \"GMMN Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95125da7",
   "metadata": {},
   "source": [
    "## Bootstrapping Hypothesis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.bootstrapping_test import bootstrap_hypothesis_test\n",
    "\n",
    "original_data = []\n",
    "for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "    original_data.append(data.cpu()) # .cpu() if data is on GPU\n",
    "original_data = torch.cat(original_data, dim=0)\n",
    "original_data = np.squeeze(original_data.numpy())\n",
    "generating_function = generate_gmmn_samples\n",
    "gen_args = (gmm_net, autoencoder, NOISE_SIZE, 20000)\n",
    "alpha = 0.05\n",
    "num_iterations = 1000\n",
    "\n",
    "bootstrap_hypothesis_test(original_data, generating_function, gen_args, alpha, num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
